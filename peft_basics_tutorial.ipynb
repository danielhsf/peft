{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH4hNM4OhXWO"
      },
      "source": [
        "# Tutorial Básico de PEFT (Parameter-Efficient Fine-Tuning)"
      ],
      "id": "hH4hNM4OhXWO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUqihQmshXWQ"
      },
      "source": [
        "## O que é PEFT?"
      ],
      "id": "HUqihQmshXWQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZISD9mFhXWR"
      },
      "source": [
        "Parameter-Efficient Fine-Tuning (PEFT) é um conjunto de técnicas para adaptar modelos de linguagem grandes (LLMs) pré-treinados a novas tarefas de forma eficiente. Em vez de treinar todos os parâmetros do modelo (fine-tuning completo), o PEFT se concentra em ajustar apenas um pequeno número de parâmetros ou adicionar um pequeno conjunto de novos parâmetros. Isso torna o processo de fine-tuning muito mais rápido e menos custoso em termos de recursos computacionais, ao mesmo tempo em que mantém ou até melhora o desempenho do modelo na tarefa alvo."
      ],
      "id": "PZISD9mFhXWR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEWtBuZChXWR"
      },
      "source": [
        "## Por que usar PEFT?"
      ],
      "id": "ZEWtBuZChXWR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRL9KsrvhXWS"
      },
      "source": [
        "Existem várias vantagens em usar técnicas PEFT:\n",
        "\n",
        "- **Custo computacional reduzido:** Treinar apenas uma pequena fração dos parâmetros requer significativamente menos poder de processamento (GPUs) e tempo.\n",
        "- **Menor uso de memória:** Como menos parâmetros são atualizados, a quantidade de memória RAM e VRAM necessária é drasticamente reduzida.\n",
        "- **Menor espaço de armazenamento para checkpoints:** Os checkpoints do modelo ajustado são muito menores, pois apenas os parâmetros modificados ou adicionados precisam ser salvos.\n",
        "- **Melhor desempenho em tarefas downstream com menos dados:** Em cenários com dados limitados para a tarefa específica, o PEFT pode superar o fine-tuning completo, pois reduz o risco de overfitting ao ajustar menos parâmetros.\n",
        "- **Compartilhamento e implantação mais fáceis:** Modelos menores e mais eficientes são mais fáceis de compartilhar e implantar em produção.\n",
        "- **Preservação do conhecimento do modelo pré-treinado:** Ao congelar a maioria dos pesos do modelo original, o PEFT ajuda a reter o vasto conhecimento aprendido durante a fase de pré-treinamento, evitando o \"esquecimento catastrófico\"."
      ],
      "id": "vRL9KsrvhXWS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3wZCiOchXWT"
      },
      "source": [
        "## Visão Geral de Técnicas PEFT Comuns"
      ],
      "id": "Q3wZCiOchXWT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hzsyxfRhXWT"
      },
      "source": [
        "Existem diversas técnicas PEFT, cada uma com sua abordagem particular para tornar o fine-tuning mais eficiente. Algumas das mais comuns incluem:\n",
        "\n",
        "- **LoRA (Low-Rank Adaptation):** Congela os pesos pré-treinados do modelo e injeta matrizes de \"decomposição de posto baixo\" (low-rank) treináveis em cada camada da arquitetura do Transformer. Isso reduz significativamente o número de parâmetros treináveis.\n",
        "- **Prefix Tuning:** Adiciona um pequeno conjunto de vetores (prefixos) treináveis à entrada de cada camada do Transformer, sem modificar os parâmetros originais do modelo. Esses prefixos são otimizados para guiar o comportamento do modelo na tarefa específica.\n",
        "- **P-Tuning (Prompt Tuning with Prompt Encoder):** Semelhante ao Prefix Tuning, mas utiliza um pequeno codificador de prompt (geralmente uma MLP) para gerar os prefixos contínuos, oferecendo mais flexibilidade.\n",
        "- **Prompt Tuning:** É a forma mais simples, onde apenas alguns tokens de prompt \"virtuais\" ou \"contínuos\" são adicionados à sequência de entrada e otimizados diretamente, mantendo todo o restante do modelo congelado."
      ],
      "id": "9hzsyxfRhXWT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmIfmtZkhXWU"
      },
      "source": [
        "## Instalação"
      ],
      "id": "GmIfmtZkhXWU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUYFsxqQhXWU"
      },
      "source": [
        "A biblioteca `peft` pode ser facilmente instalada usando o `pip`. Execute o comando abaixo em sua célula de código para instalá-la:"
      ],
      "id": "WUYFsxqQhXWU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beeCc_Y3hXWU",
        "outputId": "0d89ccc8-56e9-4171-8aeb-5e78c8ed9d25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.52.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.7.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.4.26)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.32.4)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.8/514.8 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, huggingface_hub, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.32.4\n",
            "    Uninstalling huggingface-hub-0.32.4:\n",
            "      Successfully uninstalled huggingface-hub-0.32.4\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0 huggingface_hub-0.33.0\n"
          ]
        }
      ],
      "source": [
        "!pip install peft\n",
        "!pip install -U datasets huggingface_hub fsspec"
      ],
      "execution_count": 1,
      "id": "beeCc_Y3hXWU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iefEy_QNhXWV"
      },
      "source": [
        "## Demonstração de Uso Básico do PEFT com LoRA"
      ],
      "id": "iefEy_QNhXWV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qxf9y1ZchXWV"
      },
      "source": [
        "### Configuração Inicial e Importações"
      ],
      "id": "Qxf9y1ZchXWV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERmAt5O0hXWV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from peft import LoraConfig, get_peft_model, TaskType, AutoPeftModelForSequenceClassification\n",
        "from datasets import load_dataset"
      ],
      "execution_count": 41,
      "id": "ERmAt5O0hXWV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImD_HJQAhXWV"
      },
      "source": [
        "### Carregando o Modelo Base e o Tokenizador"
      ],
      "id": "ImD_HJQAhXWV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOMG8hGFhXWV"
      },
      "source": [
        "Vamos escolher um modelo pré-treinado da Hugging Face Hub. Para este tutorial, usaremos o `distilbert-base-uncased` por ser um modelo leve e rápido para treinar. É crucial usar o tokenizador correspondente ao modelo para garantir que a entrada seja processada da maneira que o modelo espera."
      ],
      "id": "jOMG8hGFhXWV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7KtEfC4hXWV",
        "outputId": "7da185a5-f5fc-4308-fdab-51c9d85bb446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Carregamos o modelo para classificação de sequência com num_labels=2, pois o dataset IMDB é para classificação binária (positivo/negativo).\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ],
      "execution_count": 42,
      "id": "C7KtEfC4hXWV"
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kum0ghM-ogb-",
        "outputId": "e42eb425-1b55-4a4a-cda5-6e1790f889cd"
      },
      "id": "kum0ghM-ogb-",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBertForSequenceClassification(\n",
            "  (distilbert): DistilBertModel(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): Transformer(\n",
            "      (layer): ModuleList(\n",
            "        (0-5): 6 x TransformerBlock(\n",
            "          (attention): DistilBertSdpaAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (activation): GELUActivation()\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nArmmDYXhXWW"
      },
      "source": [
        "`num_labels=2` é definido porque o dataset IMDB, que usaremos a seguir, é para uma tarefa de classificação binária (sentimento positivo ou negativo)."
      ],
      "id": "nArmmDYXhXWW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1AklKrlhXWW"
      },
      "source": [
        "### Preparando o Conjunto de Dados para Classificação de Texto"
      ],
      "id": "a1AklKrlhXWW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrdEOeqohXWW"
      },
      "source": [
        "Usaremos o dataset `imdb` da biblioteca `datasets`, que contém avaliações de filmes e seus respectivos sentimentos. Para agilizar o processo no tutorial, carregaremos apenas uma pequena fração (1%) do conjunto de treino."
      ],
      "id": "yrdEOeqohXWW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "7006577638ad481d86000380fb08fc43",
            "1e5d2bfdb76444dbb02221ea1d4f9b64",
            "e1325217cd784adb84f9fe2f7b98b163",
            "585f9d93be224969abc62cd1fa97adba",
            "33a350a75f674b7fa2c747873f747948",
            "e15d1f0d371e49d7ad4694c8d5f07afb",
            "5fbc5b4f7b5a454c9061a617024e67a1",
            "1e603d10a816466daffddaafdffd7227",
            "ab05dc7adb8f4c39ae9f96a0ee640b02",
            "76baa381dc9f466facf63b4b4f60275d",
            "3377f011b8cc40609cd3e839c0b2d43d",
            "140778fcfd7748fb9c753ac953282e4b",
            "60bda788b9234885b004820d7e2b44d7",
            "102f06f310e949e2b09dfec9eb032ba6",
            "43589c8acfd44aa99f621377a0c97761",
            "2274f0519b884498b28c56348ed25e1e",
            "5bb4a350f42c410eb0162847c6f3883b",
            "489deab5d516454f8f0548fb92b9a18c",
            "9c13bc2f55054bc0a0d114511143b167",
            "8f82451a97e14cfcbd37b93e452fb9bc",
            "b3b5c6444ebe47e68f206104f86809b4",
            "894499845f2a414ba6c8dc811e319914"
          ]
        },
        "id": "XrTC3hpUhXWW",
        "outputId": "01a737a1-785c-44d1-fff9-46dfcaf2e9b2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7006577638ad481d86000380fb08fc43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "140778fcfd7748fb9c753ac953282e4b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Carrega uma pequena fração do dataset IMDB (1% do treino)\n",
        "dataset = load_dataset('imdb',split='train')\n",
        "\n",
        "# Divide o dataset carregado em treino (80%) e teste (20%)\n",
        "dataset = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "# Função para tokenizar os textos\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Aplica a tokenização ao dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove a coluna de texto original, pois não é mais necessária após a tokenização\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
        "\n",
        "# Renomeia a coluna 'label' para 'labels', que é o nome esperado pelo modelo/Trainer\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# Define o formato do dataset para tensores PyTorch\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "# Cria um DataCollator para preencher dinamicamente as sequências no batch ao comprimento máximo do batch\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "execution_count": 60,
      "id": "XrTC3hpUhXWW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4hhimQHhXWW"
      },
      "source": [
        "**Passos da preparação do dataset:**\n",
        "1. `load_dataset(\"imdb\", split=\"train[:1%]\")`: Carrega os primeiros 1% dos dados de treino do IMDB.\n",
        "2. `dataset.train_test_split(test_size=0.2)`: Divide essa pequena porção em conjuntos de treino e teste.\n",
        "3. `tokenize_function`: Define uma função para tokenizar as amostras de texto. `padding=\"max_length\"` e `truncation=True` garantem que todas as sequências tenham o mesmo comprimento.\n",
        "4. `dataset.map(tokenize_function, batched=True)`: Aplica a tokenização a todas as amostras de forma eficiente.\n",
        "5. `remove_columns([\"text\"])`: Remove a coluna de texto original, pois agora temos os `input_ids` e `attention_mask`.\n",
        "6. `rename_column(\"label\", \"labels\")`: Renomeia a coluna de rótulos para o nome esperado pelo `Trainer`.\n",
        "7. `set_format(\"torch\")`: Converte o dataset para o formato de tensores PyTorch.\n",
        "8. `DataCollatorWithPadding`: Cria um objeto que irá agrupar as amostras em lotes (batches) e aplicar padding dinâmico a cada lote durante o treinamento."
      ],
      "id": "l4hhimQHhXWW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WymnIKlyhXWW"
      },
      "source": [
        "### Definindo a Configuração PEFT (LoRA)"
      ],
      "id": "WymnIKlyhXWW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDNUpHdchXWX"
      },
      "source": [
        "Agora, vamos configurar o LoRA. A `LoraConfig` da biblioteca `peft` nos permite especificar como o LoRA deve ser aplicado. Os parâmetros importantes são:\n",
        "- `task_type`: O tipo de tarefa para a qual o modelo está sendo adaptado (ex: `SEQ_CLS` para classificação de sequência).\n",
        "- `r`: A dimensão do rank (posto) das matrizes de atualização LoRA. Um valor menor de `r` significa menos parâmetros treináveis.\n",
        "- `lora_alpha`: O fator de escala para as matrizes LoRA. É comum definir `lora_alpha` como o dobro de `r`.\n",
        "- `lora_dropout`: A probabilidade de dropout nas camadas LoRA.\n",
        "- `base_model_name_or_path`: O nome ou caminho do modelo base, necessário para algumas funcionalidades como salvar o adaptador."
      ],
      "id": "yDNUpHdchXWX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8IcdB--hXWX"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    base_model_name_or_path=model_name,# Adicionado para melhor compatibilidade ao salvar\n",
        "    target_modules=[\n",
        "        \"q_lin\",\n",
        "        \"k_lin\"]\n",
        ")\n",
        "\n",
        "# TaskType.SEQ_CLS é usado pois estamos fazendo classificação de sequência (sentimento de texto)."
      ],
      "execution_count": 61,
      "id": "t8IcdB--hXWX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oXCV_nkhXWX"
      },
      "source": [
        "`TaskType.SEQ_CLS` indica que estamos realizando uma tarefa de classificação de sequência (neste caso, análise de sentimento)."
      ],
      "id": "4oXCV_nkhXWX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4lbS92OhXWX"
      },
      "source": [
        "### Envolvendo o Modelo Base com PEFT"
      ],
      "id": "N4lbS92OhXWX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRHZp7_LhXWX",
        "outputId": "93b7561a-d254-4ef2-f3ec-88fe786503c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "peft_model = get_peft_model(model, peft_config)\n",
        "# Agora `peft_model` é o modelo que será treinado. Apenas os parâmetros LoRA serão atualizados."
      ],
      "execution_count": 62,
      "id": "hRHZp7_LhXWX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTT-SKomhXWX"
      },
      "source": [
        "### Verificando a Eficiência: Parâmetros Treináveis"
      ],
      "id": "iTT-SKomhXWX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYp3hqd8hXWX",
        "outputId": "83e233cb-cf31-4c3c-efe1-66303a72f9cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 739,586 || all params: 67,694,596 || trainable%: 1.0925\n"
          ]
        }
      ],
      "source": [
        "peft_model.print_trainable_parameters()"
      ],
      "execution_count": 63,
      "id": "nYp3hqd8hXWX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf7cloYdhXWY"
      },
      "source": [
        "Observe a saída do comando acima. Ela mostrará o número total de parâmetros no modelo e quantos deles são treináveis. Com LoRA, o número de parâmetros treináveis é uma pequena fração do total, o que demonstra a eficiência da técnica."
      ],
      "id": "vf7cloYdhXWY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uoWcuvrhXWY"
      },
      "source": [
        "### Treinando o Modelo PEFT"
      ],
      "id": "2uoWcuvrhXWY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zf5742AhXWY"
      },
      "source": [
        "Para o treinamento, usamos a classe `Trainer` da biblioteca `transformers`. Precisamos definir `TrainingArguments` que especificam vários hiperparâmetros e configurações para o ciclo de treinamento, como taxa de aprendizado, número de épocas, tamanho do batch, etc."
      ],
      "id": "8Zf5742AhXWY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "0NNg-8HphXWY",
        "outputId": "731024eb-9cf5-4b61-8415-47cd972dea1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-64-1038654705>:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 05:31, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.328900</td>\n",
              "      <td>0.276339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.244800</td>\n",
              "      <td>0.245095</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2500, training_loss=0.3343342376708984, metrics={'train_runtime': 331.9059, 'train_samples_per_second': 120.516, 'train_steps_per_second': 7.532, 'total_flos': 5389576273920000.0, 'train_loss': 0.3343342376708984, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"peft_basics_output\",          # Diretório para salvar os checkpoints e logs\n",
        "    learning_rate=2e-3,                     # Taxa de aprendizado (geralmente maior para PEFT)\n",
        "    per_device_train_batch_size=16,         # Tamanho do batch de treino por dispositivo\n",
        "    per_device_eval_batch_size=16,          # Tamanho do batch de avaliação por dispositivo\n",
        "    num_train_epochs=2,                     # Número de épocas de treinamento (pequeno para o tutorial)\n",
        "    weight_decay=0.01,                      # Força da regularização L2\n",
        "    eval_strategy=\"epoch\",                  # Avaliar ao final de cada época\n",
        "    save_strategy=\"epoch\",                  # Salvar o modelo ao final de cada época\n",
        "    load_best_model_at_end=True,            # Carregar o melhor modelo ao final do treino\n",
        "    report_to=\"none\"                        # Sem relatórios de treinamento\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,                         # O modelo PEFT para treinar\n",
        "    args=training_args,                       # Argumentos de treinamento\n",
        "    train_dataset=tokenized_datasets[\"train\"], # Dataset de treino\n",
        "    eval_dataset=tokenized_datasets[\"test\"],  # Dataset de avaliação/teste\n",
        "    tokenizer=tokenizer,                      # Tokenizador (para padding e outras funções)\n",
        "    data_collator=data_collator,              # Data collator para criar os batches\n",
        ")\n",
        "\n",
        "# Inicia o treinamento\n",
        "trainer.train()\n",
        "\n",
        "# Este processo pode levar alguns minutos, dependendo da sua máquina e da fração do dataset."
      ],
      "execution_count": 64,
      "id": "0NNg-8HphXWY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqXUVdmshXWY"
      },
      "source": [
        "O treinamento pode levar alguns minutos para ser concluído. A taxa de aprendizado (`learning_rate`) para PEFT costuma ser mais alta do que para o fine-tuning completo."
      ],
      "id": "XqXUVdmshXWY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlXaupeIhXWZ"
      },
      "source": [
        "### Inferência com o Modelo PEFT Treinado"
      ],
      "id": "qlXaupeIhXWZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTr60UGshXWZ",
        "outputId": "83691936-5362-4038-e88b-fa9642c5e457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: This is a fantastic movie!\n",
            "Predicted label: positive\n"
          ]
        }
      ],
      "source": [
        "text = \"This is a fantastic movie!\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(peft_model.device)\n",
        "\n",
        "# Garante que os inputs estão no mesmo device que o modelo (CPU ou GPU)\n",
        "# Se você estiver usando GPU, o peft_model.device será algo como 'cuda:0'\n",
        "#inputs = {k: v.to(peft_model.device) for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad(): # Desativa o cálculo de gradientes para inferência\n",
        "    logits = peft_model(**inputs).logits\n",
        "\n",
        "predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "print(f\"Input: {text}\")\n",
        "print(f\"Predicted label: {'positive' if predictions.item() == 1 else 'negative'}\")"
      ],
      "execution_count": 65,
      "id": "oTr60UGshXWZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwXa5MiahXWZ"
      },
      "source": [
        "**Explicação do código de inferência:**\n",
        "1. Tokenizamos um texto de exemplo.\n",
        "2. Movemos os tensores de entrada para o mesmo dispositivo onde o `peft_model` está (CPU ou GPU). Isso é importante para evitar erros de dispositivo.\n",
        "3. Usamos `torch.no_grad()` para desabilitar o cálculo de gradientes, pois não estamos treinando, apenas fazendo uma predição. Isso economiza memória e acelera o processo.\n",
        "4. Passamos os inputs para o `peft_model` e obtemos os `logits` (saídas brutas antes da função de ativação final).\n",
        "5. `torch.argmax` encontra o índice da classe com a maior probabilidade (0 para negativo, 1 para positivo no caso do IMDB).\n",
        "6. Imprimimos o resultado."
      ],
      "id": "KwXa5MiahXWZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RzNgrL2hXWZ"
      },
      "source": [
        "### Salvando e Carregando Adaptadores PEFT"
      ],
      "id": "6RzNgrL2hXWZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Kp1AXjOhXWZ"
      },
      "source": [
        "Uma grande vantagem do PEFT é que você só precisa salvar os pesos do adaptador treinado, que são muito pequenos em comparação com o modelo completo. A biblioteca `peft` facilita isso."
      ],
      "id": "7Kp1AXjOhXWZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcdUZn0rhXWf",
        "outputId": "94abe801-0a42-4956-d047-69b27989d249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adaptador PEFT salvo em ./peft_adapter_imdb\n",
            "Input: This movie was absolutely terrible and boring.\n",
            "Predicted label (loaded model): negative\n"
          ]
        }
      ],
      "source": [
        "peft_model_path = \"./peft_adapter_imdb\"\n",
        "\n",
        "# Salva apenas os pesos do adaptador LoRA treinados\n",
        "peft_model.save_pretrained(peft_model_path)\n",
        "\n",
        "print(f\"Adaptador PEFT salvo em {peft_model_path}\")\n",
        "\n",
        "# Para carregar o adaptador, você pode usar AutoPeftModelForSequenceClassification\n",
        "# Isso carregará automaticamente o modelo base correto (especificado na LoraConfig) e aplicará o adaptador.\n",
        "\n",
        "loaded_model = AutoPeftModelForSequenceClassification.from_pretrained(peft_model_path)\n",
        "\n",
        "# Vamos testar o modelo carregado\n",
        "test_text_loaded = \"This movie was absolutely terrible and boring.\"\n",
        "inputs_loaded = tokenizer(test_text_loaded, return_tensors=\"pt\")\n",
        "\n",
        "# Garante que os inputs estão no mesmo device que o modelo carregado\n",
        "inputs_loaded = {k: v.to(loaded_model.device) for k, v in inputs_loaded.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits_loaded = loaded_model(**inputs_loaded).logits\n",
        "\n",
        "predictions_loaded = torch.argmax(logits_loaded, dim=-1)\n",
        "\n",
        "print(f\"Input: {test_text_loaded}\")\n",
        "print(f\"Predicted label (loaded model): {'positive' if predictions_loaded.item() == 1 else 'negative'}\")"
      ],
      "execution_count": 66,
      "id": "lcdUZn0rhXWf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xVahXw2hXWf"
      },
      "source": [
        "Ao usar `AutoPeftModelForSequenceClassification.from_pretrained(peft_model_path)`, a biblioteca `peft` carrega automaticamente o modelo base especificado na configuração do adaptador (que foi salva junto com os pesos do adaptador) e, em seguida, aplica os pesos do adaptador LoRA a ele. Isso simplifica o processo de implantação e compartilhamento de modelos ajustados com PEFT."
      ],
      "id": "_xVahXw2hXWf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sAAjzTshXWf"
      },
      "source": [
        "## Outras Técnicas PEFT"
      ],
      "id": "-sAAjzTshXWf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG__MGsUhXWf"
      },
      "source": [
        "Além do LoRA, a biblioteca `peft` suporta várias outras técnicas eficientes de fine-tuning. A abordagem geral para usá-las é semelhante: você define um objeto de configuração específico da técnica (por exemplo, `PrefixTuningConfig`, `PromptEncoderConfig` ou `PromptTuningConfig`) e depois usa a função `get_peft_model` para aplicar essa configuração ao seu modelo base. Os parâmetros específicos dentro dessas configurações irão variar de acordo com a técnica.\n",
        "\n",
        "Aqui estão algumas outras técnicas populares:\n",
        "\n",
        "- **Prefix Tuning:** Esta técnica mantém os parâmetros do modelo original congelados e adiciona um pequeno conjunto de vetores (prefixos) treináveis à entrada de cada camada do Transformer. Esses prefixos são otimizados para guiar o comportamento do modelo na tarefa específica. Diferentemente do LoRA, que modifica as camadas existentes, o Prefix Tuning adiciona novos parâmetros na forma de prefixos.\n",
        "\n",
        "- **P-Tuning (Prompt Tuning with Prompt Encoder):** Similar ao Prefix Tuning, o P-Tuning também adiciona embeddings treináveis (prompts virtuais ou contínuos) à sequência de entrada. No entanto, em vez de adicionar prefixos fixos a cada camada, o P-Tuning (especialmente a versão com codificador, P-Tuning v2) pode usar um pequeno codificador de prompt (geralmente uma MLP - Multi-Layer Perceptron) para gerar esses embeddings de prompt de forma mais dinâmica e, potencialmente, aplicá-los em diferentes locais do modelo.\n",
        "\n",
        "- **Prompt Tuning:** Esta é uma forma mais simples e leve de P-Tuning. Ela adiciona um pequeno número de tokens de prompt \"virtuais\" ou \"suaves\" (soft prompts) treináveis diretamente aos embeddings de entrada da sequência. Apenas esses embeddings de prompt são otimizados, enquanto todo o resto do modelo pré-treinado permanece congelado. É uma das formas mais eficientes em termos de parâmetros, pois treina ainda menos parâmetros que o LoRA ou Prefix Tuning.\n",
        "\n",
        "A escolha da técnica PEFT pode depender da tarefa específica, do modelo base e dos recursos computacionais disponíveis. Cada uma oferece um trade-off diferente entre eficiência de parâmetros, desempenho e complexidade de implementação."
      ],
      "id": "YG__MGsUhXWf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PXooLlEhXWg"
      },
      "source": [
        "## Conclusão e Próximos Passos"
      ],
      "id": "5PXooLlEhXWg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TffrE2xahXWg"
      },
      "source": [
        "Neste tutorial, exploramos os conceitos básicos do Parameter-Efficient Fine-Tuning (PEFT) e demonstramos como usar a técnica LoRA para adaptar um modelo pré-treinado a uma tarefa de classificação de texto com um custo computacional significativamente reduzido.\n",
        "\n",
        "**Principais Vantagens do PEFT Recapituladas:**\n",
        "- **Eficiência de Parâmetros:** Apenas uma pequena fração dos parâmetros do modelo é treinada, resultando em checkpoints muito menores.\n",
        "- **Redução de Custos Computacionais:** Menos parâmetros para treinar significam menor tempo de treinamento e menor necessidade de GPUs potentes.\n",
        "- **Menor Uso de Memória:** Tanto para treinamento quanto para inferência.\n",
        "- **Preservação do Conhecimento:** Evita o esquecimento catastrófico ao manter a maioria dos pesos do modelo original congelados.\n",
        "- **Versatilidade:** Aplicável a uma ampla gama de modelos e tarefas.\n",
        "\n",
        "**Próximos Passos:**\n",
        "\n",
        "1.  **Experimente!** A melhor maneira de aprender é praticando. Tente aplicar LoRA ou outras técnicas PEFT a diferentes modelos e datasets. Ajuste hiperparâmetros como `r`, `lora_alpha`, `learning_rate`, etc., para observar o impacto no desempenho e na eficiência.\n",
        "2.  **Explore Outras Técnicas PEFT:** Investigue Prefix Tuning, P-Tuning, e Prompt Tuning usando a biblioteca `peft`. Cada uma tem suas nuances e pode ser mais adequada para determinados cenários.\n",
        "3.  **Aprofunde-se na Documentação:** Consulte os recursos oficiais para entender melhor as capacidades avançadas e as últimas atualizações.\n",
        "\n",
        "**Recursos Adicionais:**\n",
        "\n",
        "- **Documentação Oficial do PEFT (Hugging Face):** [`https://huggingface.co/docs/peft`](https://huggingface.co/docs/peft)\n",
        "- **Repositório GitHub do PEFT:** [`https://github.com/huggingface/peft`](https://github.com/huggingface/peft)\n",
        "- **Blog da Hugging Face (procure por posts sobre PEFT):** [`https://huggingface.co/blog`](https://huggingface.co/blog)\n",
        "- **Tarefas de exemplo no repositório PEFT:** O repositório GitHub do PEFT geralmente contém scripts de exemplo para diferentes tarefas e modelos.\n",
        "\n",
        "PEFT está democratizando o acesso ao fine-tuning de modelos grandes, permitindo que mais pesquisadores e desenvolvedores adaptem esses poderosos modelos às suas necessidades específicas. Continue explorando e construindo!"
      ],
      "id": "TffrE2xahXWg"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7006577638ad481d86000380fb08fc43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e5d2bfdb76444dbb02221ea1d4f9b64",
              "IPY_MODEL_e1325217cd784adb84f9fe2f7b98b163",
              "IPY_MODEL_585f9d93be224969abc62cd1fa97adba"
            ],
            "layout": "IPY_MODEL_33a350a75f674b7fa2c747873f747948"
          }
        },
        "1e5d2bfdb76444dbb02221ea1d4f9b64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e15d1f0d371e49d7ad4694c8d5f07afb",
            "placeholder": "​",
            "style": "IPY_MODEL_5fbc5b4f7b5a454c9061a617024e67a1",
            "value": "Map: 100%"
          }
        },
        "e1325217cd784adb84f9fe2f7b98b163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e603d10a816466daffddaafdffd7227",
            "max": 20000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab05dc7adb8f4c39ae9f96a0ee640b02",
            "value": 20000
          }
        },
        "585f9d93be224969abc62cd1fa97adba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76baa381dc9f466facf63b4b4f60275d",
            "placeholder": "​",
            "style": "IPY_MODEL_3377f011b8cc40609cd3e839c0b2d43d",
            "value": " 20000/20000 [00:06&lt;00:00, 2908.63 examples/s]"
          }
        },
        "33a350a75f674b7fa2c747873f747948": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e15d1f0d371e49d7ad4694c8d5f07afb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fbc5b4f7b5a454c9061a617024e67a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e603d10a816466daffddaafdffd7227": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab05dc7adb8f4c39ae9f96a0ee640b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76baa381dc9f466facf63b4b4f60275d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3377f011b8cc40609cd3e839c0b2d43d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "140778fcfd7748fb9c753ac953282e4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60bda788b9234885b004820d7e2b44d7",
              "IPY_MODEL_102f06f310e949e2b09dfec9eb032ba6",
              "IPY_MODEL_43589c8acfd44aa99f621377a0c97761"
            ],
            "layout": "IPY_MODEL_2274f0519b884498b28c56348ed25e1e"
          }
        },
        "60bda788b9234885b004820d7e2b44d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bb4a350f42c410eb0162847c6f3883b",
            "placeholder": "​",
            "style": "IPY_MODEL_489deab5d516454f8f0548fb92b9a18c",
            "value": "Map: 100%"
          }
        },
        "102f06f310e949e2b09dfec9eb032ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c13bc2f55054bc0a0d114511143b167",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f82451a97e14cfcbd37b93e452fb9bc",
            "value": 5000
          }
        },
        "43589c8acfd44aa99f621377a0c97761": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3b5c6444ebe47e68f206104f86809b4",
            "placeholder": "​",
            "style": "IPY_MODEL_894499845f2a414ba6c8dc811e319914",
            "value": " 5000/5000 [00:02&lt;00:00, 2063.19 examples/s]"
          }
        },
        "2274f0519b884498b28c56348ed25e1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bb4a350f42c410eb0162847c6f3883b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "489deab5d516454f8f0548fb92b9a18c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c13bc2f55054bc0a0d114511143b167": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f82451a97e14cfcbd37b93e452fb9bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3b5c6444ebe47e68f206104f86809b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "894499845f2a414ba6c8dc811e319914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}