{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Básico de PEFT (Parameter-Efficient Fine-Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é PEFT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter-Efficient Fine-Tuning (PEFT) é um conjunto de técnicas para adaptar modelos de linguagem grandes (LLMs) pré-treinados a novas tarefas de forma eficiente. Em vez de treinar todos os parâmetros do modelo (fine-tuning completo), o PEFT se concentra em ajustar apenas um pequeno número de parâmetros ou adicionar um pequeno conjunto de novos parâmetros. Isso torna o processo de fine-tuning muito mais rápido e menos custoso em termos de recursos computacionais, ao mesmo tempo em que mantém ou até melhora o desempenho do modelo na tarefa alvo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Por que usar PEFT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem várias vantagens em usar técnicas PEFT:\n",
    "\n",
    "- **Custo computacional reduzido:** Treinar apenas uma pequena fração dos parâmetros requer significativamente menos poder de processamento (GPUs) e tempo.\n",
    "- **Menor uso de memória:** Como menos parâmetros são atualizados, a quantidade de memória RAM e VRAM necessária é drasticamente reduzida.\n",
    "- **Menor espaço de armazenamento para checkpoints:** Os checkpoints do modelo ajustado são muito menores, pois apenas os parâmetros modificados ou adicionados precisam ser salvos.\n",
    "- **Melhor desempenho em tarefas downstream com menos dados:** Em cenários com dados limitados para a tarefa específica, o PEFT pode superar o fine-tuning completo, pois reduz o risco de overfitting ao ajustar menos parâmetros.\n",
    "- **Compartilhamento e implantação mais fáceis:** Modelos menores e mais eficientes são mais fáceis de compartilhar e implantar em produção.\n",
    "- **Preservação do conhecimento do modelo pré-treinado:** Ao congelar a maioria dos pesos do modelo original, o PEFT ajuda a reter o vasto conhecimento aprendido durante a fase de pré-treinamento, evitando o \"esquecimento catastrófico\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visão Geral de Técnicas PEFT Comuns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem diversas técnicas PEFT, cada uma com sua abordagem particular para tornar o fine-tuning mais eficiente. Algumas das mais comuns incluem:\n",
    "\n",
    "- **LoRA (Low-Rank Adaptation):** Congela os pesos pré-treinados do modelo e injeta matrizes de \"decomposição de posto baixo\" (low-rank) treináveis em cada camada da arquitetura do Transformer. Isso reduz significativamente o número de parâmetros treináveis.\n",
    "- **Prefix Tuning:** Adiciona um pequeno conjunto de vetores (prefixos) treináveis à entrada de cada camada do Transformer, sem modificar os parâmetros originais do modelo. Esses prefixos são otimizados para guiar o comportamento do modelo na tarefa específica.\n",
    "- **P-Tuning (Prompt Tuning with Prompt Encoder):** Semelhante ao Prefix Tuning, mas utiliza um pequeno codificador de prompt (geralmente uma MLP) para gerar os prefixos contínuos, oferecendo mais flexibilidade.\n",
    "- **Prompt Tuning:** É a forma mais simples, onde apenas alguns tokens de prompt \"virtuais\" ou \"contínuos\" são adicionados à sequência de entrada e otimizados diretamente, mantendo todo o restante do modelo congelado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biblioteca `peft` pode ser facilmente instalada usando o `pip`. Execute o comando abaixo em sua célula de código para instalá-la:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstração de Uso Básico do PEFT com LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração Inicial e Importações"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from peft import LoraConfig, get_peft_model, TaskType, AutoPeftModelForSequenceClassification\n",
    "from datasets import load_dataset"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o Modelo Base e o Tokenizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos escolher um modelo pré-treinado da Hugging Face Hub. Para este tutorial, usaremos o `distilbert-base-uncased` por ser um modelo leve e rápido para treinar. É crucial usar o tokenizador correspondente ao modelo para garantir que a entrada seja processada da maneira que o modelo espera."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Carregamos o modelo para classificação de sequência com num_labels=2, pois o dataset IMDB é para classificação binária (positivo/negativo).\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num_labels=2` é definido porque o dataset IMDB, que usaremos a seguir, é para uma tarefa de classificação binária (sentimento positivo ou negativo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando o Conjunto de Dados para Classificação de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos o dataset `imdb` da biblioteca `datasets`, que contém avaliações de filmes e seus respectivos sentimentos. Para agilizar o processo no tutorial, carregaremos apenas uma pequena fração (1%) do conjunto de treino."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega uma pequena fração do dataset IMDB (1% do treino)\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:1%]\")\n",
    "\n",
    "# Divide o dataset carregado em treino (80%) e teste (20%)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Função para tokenizar os textos\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Aplica a tokenização ao dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove a coluna de texto original, pois não é mais necessária após a tokenização\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "# Renomeia a coluna 'label' para 'labels', que é o nome esperado pelo modelo/Trainer\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Define o formato do dataset para tensores PyTorch\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Cria um DataCollator para preencher dinamicamente as sequências no batch ao comprimento máximo do batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Passos da preparação do dataset:**\n",
    "1. `load_dataset(\"imdb\", split=\"train[:1%]\")`: Carrega os primeiros 1% dos dados de treino do IMDB.\n",
    "2. `dataset.train_test_split(test_size=0.2)`: Divide essa pequena porção em conjuntos de treino e teste.\n",
    "3. `tokenize_function`: Define uma função para tokenizar as amostras de texto. `padding=\"max_length\"` e `truncation=True` garantem que todas as sequências tenham o mesmo comprimento.\n",
    "4. `dataset.map(tokenize_function, batched=True)`: Aplica a tokenização a todas as amostras de forma eficiente.\n",
    "5. `remove_columns([\"text\"])`: Remove a coluna de texto original, pois agora temos os `input_ids` e `attention_mask`.\n",
    "6. `rename_column(\"label\", \"labels\")`: Renomeia a coluna de rótulos para o nome esperado pelo `Trainer`.\n",
    "7. `set_format(\"torch\")`: Converte o dataset para o formato de tensores PyTorch.\n",
    "8. `DataCollatorWithPadding`: Cria um objeto que irá agrupar as amostras em lotes (batches) e aplicar padding dinâmico a cada lote durante o treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo a Configuração PEFT (LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos configurar o LoRA. A `LoraConfig` da biblioteca `peft` nos permite especificar como o LoRA deve ser aplicado. Os parâmetros importantes são:\n",
    "- `task_type`: O tipo de tarefa para a qual o modelo está sendo adaptado (ex: `SEQ_CLS` para classificação de sequência).\n",
    "- `r`: A dimensão do rank (posto) das matrizes de atualização LoRA. Um valor menor de `r` significa menos parâmetros treináveis.\n",
    "- `lora_alpha`: O fator de escala para as matrizes LoRA. É comum definir `lora_alpha` como o dobro de `r`.\n",
    "- `lora_dropout`: A probabilidade de dropout nas camadas LoRA.\n",
    "- `base_model_name_or_path`: O nome ou caminho do modelo base, necessário para algumas funcionalidades como salvar o adaptador."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, \n",
    "    r=8, \n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.1,\n",
    "    base_model_name_or_path=model_name # Adicionado para melhor compatibilidade ao salvar\n",
    ")\n",
    "\n",
    "# TaskType.SEQ_CLS é usado pois estamos fazendo classificação de sequência (sentimento de texto)."
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TaskType.SEQ_CLS` indica que estamos realizando uma tarefa de classificação de sequência (neste caso, análise de sentimento)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Envolvendo o Modelo Base com PEFT"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, peft_config)\n",
    "# Agora `peft_model` é o modelo que será treinado. Apenas os parâmetros LoRA serão atualizados."
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando a Eficiência: Parâmetros Treináveis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.print_trainable_parameters()"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe a saída do comando acima. Ela mostrará o número total de parâmetros no modelo e quantos deles são treináveis. Com LoRA, o número de parâmetros treináveis é uma pequena fração do total, o que demonstra a eficiência da técnica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando o Modelo PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o treinamento, usamos a classe `Trainer` da biblioteca `transformers`. Precisamos definir `TrainingArguments` que especificam vários hiperparâmetros e configurações para o ciclo de treinamento, como taxa de aprendizado, número de épocas, tamanho do batch, etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"peft_basics_output\",          # Diretório para salvar os checkpoints e logs\n",
    "    learning_rate=2e-3,                     # Taxa de aprendizado (geralmente maior para PEFT)\n",
    "    per_device_train_batch_size=16,         # Tamanho do batch de treino por dispositivo\n",
    "    per_device_eval_batch_size=16,          # Tamanho do batch de avaliação por dispositivo\n",
    "    num_train_epochs=2,                     # Número de épocas de treinamento (pequeno para o tutorial)\n",
    "    weight_decay=0.01,                      # Força da regularização L2\n",
    "    evaluation_strategy=\"epoch\",            # Avaliar ao final de cada época\n",
    "    save_strategy=\"epoch\",                  # Salvar o modelo ao final de cada época\n",
    "    load_best_model_at_end=True,            # Carregar o melhor modelo ao final do treino\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,                         # O modelo PEFT para treinar\n",
    "    args=training_args,                       # Argumentos de treinamento\n",
    "    train_dataset=tokenized_datasets[\"train\"], # Dataset de treino\n",
    "    eval_dataset=tokenized_datasets[\"test\"],  # Dataset de avaliação/teste\n",
    "    tokenizer=tokenizer,                      # Tokenizador (para padding e outras funções)\n",
    "    data_collator=data_collator,              # Data collator para criar os batches\n",
    ")\n",
    "\n",
    "# Inicia o treinamento\n",
    "trainer.train()\n",
    "\n",
    "# Este processo pode levar alguns minutos, dependendo da sua máquina e da fração do dataset."
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O treinamento pode levar alguns minutos para ser concluído. A taxa de aprendizado (`learning_rate`) para PEFT costuma ser mais alta do que para o fine-tuning completo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferência com o Modelo PEFT Treinado"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a fantastic movie!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Garante que os inputs estão no mesmo device que o modelo (CPU ou GPU)\n",
    "# Se você estiver usando GPU, o peft_model.device será algo como 'cuda:0'\n",
    "inputs = {k: v.to(peft_model.device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad(): # Desativa o cálculo de gradientes para inferência\n",
    "    logits = peft_model(**inputs).logits\n",
    "\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "print(f\"Input: {text}\")\n",
    "print(f\"Predicted label: {'positive' if predictions.item() == 1 else 'negative'}\")"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicação do código de inferência:**\n",
    "1. Tokenizamos um texto de exemplo.\n",
    "2. Movemos os tensores de entrada para o mesmo dispositivo onde o `peft_model` está (CPU ou GPU). Isso é importante para evitar erros de dispositivo.\n",
    "3. Usamos `torch.no_grad()` para desabilitar o cálculo de gradientes, pois não estamos treinando, apenas fazendo uma predição. Isso economiza memória e acelera o processo.\n",
    "4. Passamos os inputs para o `peft_model` e obtemos os `logits` (saídas brutas antes da função de ativação final).\n",
    "5. `torch.argmax` encontra o índice da classe com a maior probabilidade (0 para negativo, 1 para positivo no caso do IMDB).\n",
    "6. Imprimimos o resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando e Carregando Adaptadores PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma grande vantagem do PEFT é que você só precisa salvar os pesos do adaptador treinado, que são muito pequenos em comparação com o modelo completo. A biblioteca `peft` facilita isso."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_path = \"./peft_adapter_imdb\"\n",
    "\n",
    "# Salva apenas os pesos do adaptador LoRA treinados\n",
    "peft_model.save_pretrained(peft_model_path)\n",
    "\n",
    "print(f\"Adaptador PEFT salvo em {peft_model_path}\")\n",
    "\n",
    "# Para carregar o adaptador, você pode usar AutoPeftModelForSequenceClassification\n",
    "# Isso carregará automaticamente o modelo base correto (especificado na LoraConfig) e aplicará o adaptador.\n",
    "\n",
    "loaded_model = AutoPeftModelForSequenceClassification.from_pretrained(peft_model_path)\n",
    "\n",
    "# Vamos testar o modelo carregado\n",
    "test_text_loaded = \"This movie was absolutely terrible and boring.\"\n",
    "inputs_loaded = tokenizer(test_text_loaded, return_tensors=\"pt\")\n",
    "\n",
    "# Garante que os inputs estão no mesmo device que o modelo carregado\n",
    "inputs_loaded = {k: v.to(loaded_model.device) for k, v in inputs_loaded.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_loaded = loaded_model(**inputs_loaded).logits\n",
    "\n",
    "predictions_loaded = torch.argmax(logits_loaded, dim=-1)\n",
    "\n",
    "print(f\"Input: {test_text_loaded}\")\n",
    "print(f\"Predicted label (loaded model): {'positive' if predictions_loaded.item() == 1 else 'negative'}\")"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao usar `AutoPeftModelForSequenceClassification.from_pretrained(peft_model_path)`, a biblioteca `peft` carrega automaticamente o modelo base especificado na configuração do adaptador (que foi salva junto com os pesos do adaptador) e, em seguida, aplica os pesos do adaptador LoRA a ele. Isso simplifica o processo de implantação e compartilhamento de modelos ajustados com PEFT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outras Técnicas PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além do LoRA, a biblioteca `peft` suporta várias outras técnicas eficientes de fine-tuning. A abordagem geral para usá-las é semelhante: você define um objeto de configuração específico da técnica (por exemplo, `PrefixTuningConfig`, `PromptEncoderConfig` ou `PromptTuningConfig`) e depois usa a função `get_peft_model` para aplicar essa configuração ao seu modelo base. Os parâmetros específicos dentro dessas configurações irão variar de acordo com a técnica.\n",
    "\n",
    "Aqui estão algumas outras técnicas populares:\n",
    "\n",
    "- **Prefix Tuning:** Esta técnica mantém os parâmetros do modelo original congelados e adiciona um pequeno conjunto de vetores (prefixos) treináveis à entrada de cada camada do Transformer. Esses prefixos são otimizados para guiar o comportamento do modelo na tarefa específica. Diferentemente do LoRA, que modifica as camadas existentes, o Prefix Tuning adiciona novos parâmetros na forma de prefixos.\n",
    "\n",
    "- **P-Tuning (Prompt Tuning with Prompt Encoder):** Similar ao Prefix Tuning, o P-Tuning também adiciona embeddings treináveis (prompts virtuais ou contínuos) à sequência de entrada. No entanto, em vez de adicionar prefixos fixos a cada camada, o P-Tuning (especialmente a versão com codificador, P-Tuning v2) pode usar um pequeno codificador de prompt (geralmente uma MLP - Multi-Layer Perceptron) para gerar esses embeddings de prompt de forma mais dinâmica e, potencialmente, aplicá-los em diferentes locais do modelo.\n",
    "\n",
    "- **Prompt Tuning:** Esta é uma forma mais simples e leve de P-Tuning. Ela adiciona um pequeno número de tokens de prompt \"virtuais\" ou \"suaves\" (soft prompts) treináveis diretamente aos embeddings de entrada da sequência. Apenas esses embeddings de prompt são otimizados, enquanto todo o resto do modelo pré-treinado permanece congelado. É uma das formas mais eficientes em termos de parâmetros, pois treina ainda menos parâmetros que o LoRA ou Prefix Tuning.\n",
    "\n",
    "A escolha da técnica PEFT pode depender da tarefa específica, do modelo base e dos recursos computacionais disponíveis. Cada uma oferece um trade-off diferente entre eficiência de parâmetros, desempenho e complexidade de implementação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão e Próximos Passos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste tutorial, exploramos os conceitos básicos do Parameter-Efficient Fine-Tuning (PEFT) e demonstramos como usar a técnica LoRA para adaptar um modelo pré-treinado a uma tarefa de classificação de texto com um custo computacional significativamente reduzido. \n",
    "\n",
    "**Principais Vantagens do PEFT Recapituladas:**\n",
    "- **Eficiência de Parâmetros:** Apenas uma pequena fração dos parâmetros do modelo é treinada, resultando em checkpoints muito menores.\n",
    "- **Redução de Custos Computacionais:** Menos parâmetros para treinar significam menor tempo de treinamento e menor necessidade de GPUs potentes.\n",
    "- **Menor Uso de Memória:** Tanto para treinamento quanto para inferência.\n",
    "- **Preservação do Conhecimento:** Evita o esquecimento catastrófico ao manter a maioria dos pesos do modelo original congelados.\n",
    "- **Versatilidade:** Aplicável a uma ampla gama de modelos e tarefas.\n",
    "\n",
    "**Próximos Passos:**\n",
    "\n",
    "1.  **Experimente!** A melhor maneira de aprender é praticando. Tente aplicar LoRA ou outras técnicas PEFT a diferentes modelos e datasets. Ajuste hiperparâmetros como `r`, `lora_alpha`, `learning_rate`, etc., para observar o impacto no desempenho e na eficiência.\n",
    "2.  **Explore Outras Técnicas PEFT:** Investigue Prefix Tuning, P-Tuning, e Prompt Tuning usando a biblioteca `peft`. Cada uma tem suas nuances e pode ser mais adequada para determinados cenários.\n",
    "3.  **Aprofunde-se na Documentação:** Consulte os recursos oficiais para entender melhor as capacidades avançadas e as últimas atualizações.\n",
    "\n",
    "**Recursos Adicionais:**\n",
    "\n",
    "- **Documentação Oficial do PEFT (Hugging Face):** [`https://huggingface.co/docs/peft`](https://huggingface.co/docs/peft)\n",
    "- **Repositório GitHub do PEFT:** [`https://github.com/huggingface/peft`](https://github.com/huggingface/peft)\n",
    "- **Blog da Hugging Face (procure por posts sobre PEFT):** [`https://huggingface.co/blog`](https://huggingface.co/blog)\n",
    "- **Tarefas de exemplo no repositório PEFT:** O repositório GitHub do PEFT geralmente contém scripts de exemplo para diferentes tarefas e modelos.\n",
    "\n",
    "PEFT está democratizando o acesso ao fine-tuning de modelos grandes, permitindo que mais pesquisadores e desenvolvedores adaptem esses poderosos modelos às suas necessidades específicas. Continue explorando e construindo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
