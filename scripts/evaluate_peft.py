#!/usr/bin/env python3
"""
Script para avalia√ß√£o de modelos PEFT
Avalia performance e compara com modelo base
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from peft import PeftModel, PeftConfig
import time
import psutil
import numpy as np
from typing import List, Dict, Any

def load_peft_model(base_model_name: str, peft_model_path: str):
    """Carrega modelo PEFT"""
    print(f"üì• Carregando modelo PEFT de: {peft_model_path}")
    
    # Carrega configura√ß√£o PEFT
    config = PeftConfig.from_pretrained(peft_model_path)
    
    # Carrega modelo base
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    # Carrega adaptadores PEFT
    model = PeftModel.from_pretrained(base_model, peft_model_path)
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    
    return model, tokenizer

def measure_memory_usage():
    """Mede uso de mem√≥ria"""
    process = psutil.Process()
    memory_info = process.memory_info()
    return {
        'rss_mb': memory_info.rss / 1024 / 1024,
        'vms_mb': memory_info.vms / 1024 / 1024
    }

def generate_text(model, tokenizer, prompt: str, max_length: int = 100):
    """Gera texto com o modelo"""
    inputs = tokenizer(prompt, return_tensors="pt")
    
    start_time = time.time()
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    generation_time = time.time() - start_time
    
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return generated_text, generation_time

def evaluate_model(model, tokenizer, test_prompts: List[str]):
    """Avalia modelo com prompts de teste"""
    results = []
    
    print("üîç Avaliando modelo...")
    
    for i, prompt in enumerate(test_prompts):
        print(f"  Teste {i+1}/{len(test_prompts)}: {prompt[:50]}...")
        
        # Mede mem√≥ria antes
        memory_before = measure_memory_usage()
        
        # Gera texto
        generated_text, generation_time = generate_text(model, tokenizer, prompt)
        
        # Mede mem√≥ria depois
        memory_after = measure_memory_usage()
        
        # Calcula m√©tricas
        input_tokens = len(tokenizer.encode(prompt))
        output_tokens = len(tokenizer.encode(generated_text)) - input_tokens
        tokens_per_second = output_tokens / generation_time if generation_time > 0 else 0
        
        result = {
            'prompt': prompt,
            'generated_text': generated_text,
            'generation_time': generation_time,
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'tokens_per_second': tokens_per_second,
            'memory_before_mb': memory_before['rss_mb'],
            'memory_after_mb': memory_after['rss_mb'],
            'memory_increase_mb': memory_after['rss_mb'] - memory_before['rss_mb']
        }
        
        results.append(result)
    
    return results

def print_evaluation_summary(results: List[Dict[str, Any]]):
    """Imprime resumo da avalia√ß√£o"""
    print("\n" + "="*60)
    print("üìä RESUMO DA AVALIA√á√ÉO")
    print("="*60)
    
    # Estat√≠sticas de tempo
    generation_times = [r['generation_time'] for r in results]
    tokens_per_second = [r['tokens_per_second'] for r in results]
    
    print(f"‚è±Ô∏è  Tempo m√©dio de gera√ß√£o: {np.mean(generation_times):.3f}s")
    print(f"‚ö° Tokens por segundo: {np.mean(tokens_per_second):.2f}")
    
    # Estat√≠sticas de mem√≥ria
    memory_increases = [r['memory_increase_mb'] for r in results]
    print(f"üíæ Aumento m√©dio de mem√≥ria: {np.mean(memory_increases):.2f} MB")
    
    # Estat√≠sticas de tokens
    total_input_tokens = sum(r['input_tokens'] for r in results)
    total_output_tokens = sum(r['output_tokens'] for r in results)
    print(f"üìù Total de tokens de entrada: {total_input_tokens}")
    print(f"üìù Total de tokens de sa√≠da: {total_output_tokens}")
    
    print("\nüìã Exemplos de gera√ß√£o:")
    for i, result in enumerate(results[:3]):  # Mostra apenas os 3 primeiros
        print(f"\n  Exemplo {i+1}:")
        print(f"    Prompt: {result['prompt'][:100]}...")
        print(f"    Gera√ß√£o: {result['generated_text'][:200]}...")

def main():
    """Fun√ß√£o principal"""
    # Configura√ß√µes
    base_model_name = "microsoft/DialoGPT-medium"
    peft_model_path = "./models/peft_model"
    
    # Prompts de teste
    test_prompts = [
        "Ol√°, como voc√™ est√°?",
        "Conte-me uma hist√≥ria sobre tecnologia.",
        "Qual √© sua opini√£o sobre intelig√™ncia artificial?",
        "Explique o que √© machine learning.",
        "D√™-me algumas dicas para programar em Python."
    ]
    
    try:
        # Carrega modelo
        model, tokenizer = load_peft_model(base_model_name, peft_model_path)
        
        # Avalia modelo
        results = evaluate_model(model, tokenizer, test_prompts)
        
        # Imprime resultados
        print_evaluation_summary(results)
        
    except FileNotFoundError:
        print("‚ùå Modelo PEFT n√£o encontrado!")
        print("Execute primeiro o script de treinamento: python scripts/train_peft.py")
    except Exception as e:
        print(f"‚ùå Erro durante avalia√ß√£o: {e}")

if __name__ == "__main__":
    main() 