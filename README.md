# PEFT - Parameter-Efficient Fine-Tuning

## DefiniÃ§Ã£o

PEFT (Parameter-Efficient Fine-Tuning ou <i>Ajuste Fino Eficiente em ParÃ¢metros</i>) Ã© uma biblioteca para adaptar eficientemente Large Language Models (LLMs) prÃ©-treinados a diversas aplicaÃ§Ãµes especÃ­ficas (downstream tasks) sem a necessidade de realizar o ajuste fino de todos os parÃ¢metros do modelo, pois isso tem um alto custo. Os mÃ©todos PEFT realizam o ajuste fino apenas de um pequeno nÃºmero de parÃ¢metros (adicionais ou nÃ£o) do modelo â€” diminuindo significativamente os custos computacionais e de armazenamento â€” enquanto proporcionam um desempenho comparÃ¡vel ao de um modelo totalmente ajustado (fully fine-tuned). Isso torna mais acessÃ­vel treinar e armazenar grandes modelos de linguagem (LLMs) em hardware de consumidor.

O PEFT estÃ¡ integrado Ã s bibliotecas Transformers, Diffusers e Accelerate para oferecer uma forma mais rÃ¡pida e fÃ¡cil de carregar, treinar e usar grandes modelos para inferÃªncia.

## ğŸ“ Estrutura do Projeto

```
peft/
â”œâ”€â”€ notebooks/                    # Tutoriais e experimentos
â”‚   â”œâ”€â”€ peft_basics_tutorial.ipynb
â”‚   â”œâ”€â”€ PEFT_Quicktour.ipynb
â”‚   â”œâ”€â”€ Quantized_Low_Rank_Adaptation_(QLoRA).ipynb
â”‚   â””â”€â”€ M2M100_com_Peft.ipynb
â”œâ”€â”€ data/                        # Dados de treinamento
â”œâ”€â”€ models/                      # Modelos salvos
â”œâ”€â”€ scripts/                     # Scripts Python
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ Como Usar

### InstalaÃ§Ã£o

```bash
pip install -r requirements.txt
```

### Executando os Tutoriais

1. **PEFT Basics**: `peft_basics_tutorial.ipynb`
   - IntroduÃ§Ã£o aos conceitos bÃ¡sicos do PEFT
   - LoRA (Low-Rank Adaptation)
   - Prefix Tuning

2. **PEFT Quick Tour**: `PEFT_Quicktour.ipynb`
   - VisÃ£o geral rÃ¡pida das funcionalidades
   - Exemplos prÃ¡ticos

3. **QLoRA**: `Quantized_Low_Rank_Adaptation_(QLoRA).ipynb`
   - Fine-tuning quantizado
   - OtimizaÃ§Ã£o de memÃ³ria

4. **M2M100 com PEFT**: `M2M100_com_Peft.ipynb`
   - AplicaÃ§Ã£o em modelos multilingues

## ğŸ”§ MÃ©todos PEFT Suportados

- **LoRA** (Low-Rank Adaptation)
- **Prefix Tuning**
- **P-Tuning**
- **Prompt Tuning**
- **AdaLoRA**
- **QLoRA** (Quantized LoRA)

## ğŸ“Š Monitoramento e AvaliaÃ§Ã£o

- Use `accelerate` para treinamento distribuÃ­do
- Monitore mÃ©tricas com `wandb` ou `tensorboard`
- Avalie com `sacrebleu` para tarefas de traduÃ§Ã£o

## ğŸ’¡ PrÃ³ximos Passos Recomendados

1. Experimente diferentes mÃ©todos PEFT
2. Compare performance vs. custo computacional
3. Aplique em seus prÃ³prios dados
4. Explore quantizaÃ§Ã£o para otimizaÃ§Ã£o de memÃ³ria